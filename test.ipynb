{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "38422bbb",
   "metadata": {},
   "source": [
    "# **TESTING CUSTOM RANDOM FOREST IMPLEMENTATION**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc65fb40",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3f391f26",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.utils._param_validation import StrOptions\n",
    "\n",
    "from sklearn.ensemble._forest import ForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from imblearn.over_sampling import RandomOverSampler, SMOTE, BorderlineSMOTE\n",
    "from sklearn.base import clone\n",
    "from sklearn.ensemble._base import _set_random_states\n",
    "\n",
    "def _unwrap_data(X, y, sample_weight):\n",
    "    if sample_weight is None:\n",
    "        return X, y\n",
    "\n",
    "    return np.repeat(X, sample_weight.astype(int), axis=0), np.repeat(y, sample_weight.astype(int), axis=0)\n",
    "\n",
    "class OSRandomForestClassifier(ForestClassifier):\n",
    "\n",
    "    _parameter_constraints: dict = {\n",
    "        **ForestClassifier._parameter_constraints,\n",
    "        **DecisionTreeClassifier._parameter_constraints,\n",
    "        \"class_weight\": [\n",
    "            StrOptions({\"balanced_subsample\", \"balanced\"}),\n",
    "            dict,\n",
    "            list,\n",
    "            None,\n",
    "        ],\n",
    "    }\n",
    "    _parameter_constraints.pop(\"splitter\")\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        oversampling_strategy=\"random\",\n",
    "        print_indices_list=None,\n",
    "        n_estimators=100,\n",
    "        *,\n",
    "        criterion=\"gini\",\n",
    "        max_depth=None,\n",
    "        min_samples_split=2,\n",
    "        min_samples_leaf=1,\n",
    "        min_weight_fraction_leaf=0.0,\n",
    "        max_features=\"sqrt\",\n",
    "        max_leaf_nodes=None,\n",
    "        min_impurity_decrease=0.0,\n",
    "        bootstrap=True,\n",
    "        oob_score=False,\n",
    "        n_jobs=None,\n",
    "        random_state=None,\n",
    "        verbose=0,\n",
    "        warm_start=False,\n",
    "        class_weight=None,\n",
    "        ccp_alpha=0.0,\n",
    "        max_samples=None,\n",
    "        monotonic_cst=None,\n",
    "    ):\n",
    "        super().__init__(\n",
    "            estimator=OSDecisionTreeClassifier(\n",
    "                oversampling_strategy=oversampling_strategy,\n",
    "                print_var=False,\n",
    "            ),\n",
    "            n_estimators=n_estimators,\n",
    "            estimator_params=(\n",
    "                \"criterion\",\n",
    "                \"max_depth\",\n",
    "                \"min_samples_split\",\n",
    "                \"min_samples_leaf\",\n",
    "                \"min_weight_fraction_leaf\",\n",
    "                \"max_features\",\n",
    "                \"max_leaf_nodes\",\n",
    "                \"min_impurity_decrease\",\n",
    "                \"random_state\",\n",
    "                \"ccp_alpha\",\n",
    "                \"monotonic_cst\",\n",
    "            ),\n",
    "            bootstrap=bootstrap,\n",
    "            oob_score=oob_score,\n",
    "            n_jobs=n_jobs,\n",
    "            random_state=random_state,\n",
    "            verbose=verbose,\n",
    "            warm_start=warm_start,\n",
    "            class_weight=class_weight,\n",
    "            max_samples=max_samples,\n",
    "        )\n",
    "\n",
    "        self.criterion = criterion\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.min_samples_leaf = min_samples_leaf\n",
    "        self.min_weight_fraction_leaf = min_weight_fraction_leaf\n",
    "        self.max_features = max_features\n",
    "        self.max_leaf_nodes = max_leaf_nodes\n",
    "        self.min_impurity_decrease = min_impurity_decrease\n",
    "        self.monotonic_cst = monotonic_cst\n",
    "        self.ccp_alpha = ccp_alpha\n",
    "        self.oversampling_strategy = oversampling_strategy\n",
    "        self.current_tree_count = 0\n",
    "        self.print_indices_list = print_indices_list if print_indices_list is not None else []\n",
    "\n",
    "    def _make_estimator(self, append=True, random_state=None):\n",
    "        self.current_tree_count += 1\n",
    "        tree_index = self.current_tree_count\n",
    "        print_var = tree_index in self.print_indices_list\n",
    "        estimator = clone(self.estimator_)\n",
    "        estimator.set_params(**{p: getattr(self, p) for p in self.estimator_params},\n",
    "                             print_var=print_var\n",
    "                             )\n",
    "\n",
    "        if random_state is not None:\n",
    "            _set_random_states(estimator, random_state)\n",
    "\n",
    "        if append:\n",
    "            self.estimators_.append(estimator)\n",
    "\n",
    "        return estimator\n",
    "\n",
    "\n",
    "class OSDecisionTreeClassifier(DecisionTreeClassifier):\n",
    "    def __init__(\n",
    "        self,\n",
    "        *,\n",
    "        print_var=False,\n",
    "        oversampling_strategy=\"random\",\n",
    "        criterion=\"gini\",\n",
    "        splitter=\"best\",\n",
    "        max_depth=None,\n",
    "        min_samples_split=2,\n",
    "        min_samples_leaf=1,\n",
    "        min_weight_fraction_leaf=0.0,\n",
    "        max_features=None,\n",
    "        random_state=None,\n",
    "        max_leaf_nodes=None,\n",
    "        min_impurity_decrease=0.0,\n",
    "        class_weight=None,\n",
    "        ccp_alpha=0.0,\n",
    "        monotonic_cst=None,\n",
    "    ):\n",
    "        # print(f\"Oversampling strategy: {oversampling_strategy}\")\n",
    "        super().__init__(\n",
    "            criterion=criterion,\n",
    "            splitter=splitter,\n",
    "            max_depth=max_depth,\n",
    "            min_samples_split=min_samples_split,\n",
    "            min_samples_leaf=min_samples_leaf,\n",
    "            min_weight_fraction_leaf=min_weight_fraction_leaf,\n",
    "            max_features=max_features,\n",
    "            random_state=random_state,\n",
    "            max_leaf_nodes=max_leaf_nodes,\n",
    "            min_impurity_decrease=min_impurity_decrease,\n",
    "            class_weight=class_weight,\n",
    "            ccp_alpha=ccp_alpha,\n",
    "            monotonic_cst=monotonic_cst,\n",
    "        )\n",
    "        self.oversampling_strategy = oversampling_strategy\n",
    "        self.print_var = print_var\n",
    "\n",
    "    def _fit(\n",
    "        self,\n",
    "        X,\n",
    "        y,\n",
    "        sample_weight=None,\n",
    "        check_input=True,\n",
    "        missing_values_in_feature_mask=None,\n",
    "    ):\n",
    "        if self.oversampling_strategy == \"random\":\n",
    "            sampler = RandomOverSampler(random_state=self.random_state)\n",
    "        elif self.oversampling_strategy == \"SMOTE\":\n",
    "            sampler = SMOTE(random_state=self.random_state)\n",
    "            # print(f\"Using SMOTE with random_state={self.random_state} to generate synthetic samples.\")\n",
    "        elif  self.oversampling_strategy == \"BorderlineSMOTE\":\n",
    "            sampler = BorderlineSMOTE(random_state=self.random_state)\n",
    "            # print(f\"Using BorderlineSMOTE with random_state={self.random_state} to generate synthetic samples.\")\n",
    "        elif self.oversampling_strategy == \"ADASYN\":\n",
    "            sampler = ADASYN(random_state=self.random_state)\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                f\"Oversampling strategy {self.oversampling_strategy} is not supported.\"\n",
    "            )\n",
    "\n",
    "        X_drawn, y_drawn = _unwrap_data(X, y, sample_weight)\n",
    "        X_resampled, y_resampled = sampler.fit_resample(X_drawn, y_drawn)\n",
    "        count_1 = sum(y_resampled == 1)\n",
    "        count_0 = sum(y_resampled == 0)\n",
    "        # print(f\"Count of 1s: {count_1}, Count of 0s: {count_0}\")\n",
    "\n",
    "        sample_weight = [1] * len(X_drawn) + [0.5] * (len(X_resampled) - len(X_drawn))\n",
    "        # sample_weight = None\n",
    "\n",
    "        # print(f\"len X_drawn: {len(X_drawn)}\")\n",
    "        # print(f\"len X_resampled: {len(X_resampled)}\")\n",
    "        # print(X_drawn[:6, 0])\n",
    "        # print(X_resampled[:6, 0])\n",
    "\n",
    "        # if self.print_var:\n",
    "        #     print(\"Pekaboo!\")\n",
    "        # else:\n",
    "        #     print(\"Nope, not this time!\")\n",
    "\n",
    "        return super()._fit(\n",
    "            # X, y,\n",
    "            X_resampled,\n",
    "            y_resampled,\n",
    "            sample_weight=sample_weight,\n",
    "            check_input=check_input,\n",
    "            missing_values_in_feature_mask=missing_values_in_feature_mask,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dd9c4f0",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d3f23218",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== RF with SMOTE ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.96      0.97      0.96       554\n",
      "           1       0.57      0.51      0.54        45\n",
      "\n",
      "    accuracy                           0.93       599\n",
      "   macro avg       0.77      0.74      0.75       599\n",
      "weighted avg       0.93      0.93      0.93       599\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from imblearn.datasets import fetch_datasets\n",
    "us_crime = fetch_datasets()['us_crime']\n",
    "\n",
    "X = us_crime.data\n",
    "y = us_crime.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, stratify=y, test_size=0.3, random_state=42\n",
    ")\n",
    "\n",
    "rf = OSRandomForestClassifier(oversampling_strategy='SMOTE', random_state=42)\n",
    "rf.fit(X_train, y_train)\n",
    "y_pred = rf.predict(X_test)\n",
    "\n",
    "print(\"=== RF with SMOTE ===\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad896d15",
   "metadata": {},
   "source": [
    "# **TESTING MODEL COMPARATOR**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52ca8db7",
   "metadata": {},
   "source": [
    "# Comparator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "91ea0def",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.datasets import fetch_datasets\n",
    "from imblearn.over_sampling import RandomOverSampler, SMOTE, BorderlineSMOTE, ADASYN\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from os_sklearn.ensemble._forest import OSRandomForestClassifier\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from umap import UMAP\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "class Comparator:\n",
    "    def __init__(\n",
    "            self,\n",
    "            datasets=[[fetch_datasets()['us_crime'].data, fetch_datasets()['us_crime'].target],\n",
    "                      [fetch_datasets()['letter_img'].data, fetch_datasets()['letter_img'].target]],\n",
    "            test_size=0.2,\n",
    "            oversampling_strategies=['random', 'SMOTE', 'BorderlineSMOTE', 'ADASYN'], # 'random', 'SMOTE', 'BorderlineSMOTE', 'ADASYN'\n",
    "            metrics=['precision', 'recall', 'f1-score', 'accuracy'], # 'precision', 'recall', 'f1-score', 'accuracy'\n",
    "            n_trees=100,\n",
    "            iterations=100,\n",
    "            print_indices_list=[[0]] + [[]] * 99,\n",
    "            dataset_name=['us_crime', 'letter_img'],\n",
    "            mode='both' # 'both', 'bagging', 'augmentation'\n",
    "    ):\n",
    "        self.datasets = datasets\n",
    "        self.test_size = test_size\n",
    "        self.oversampling_strategies = oversampling_strategies\n",
    "        self.metrics = metrics\n",
    "        self.n_trees = n_trees\n",
    "        self.iterations = iterations\n",
    "        self.print_indices_list = print_indices_list\n",
    "        self.dataset_name = dataset_name\n",
    "        self.mode = mode\n",
    "        self.visuals = sum(len(indices) for indices in self.print_indices_list if indices)\n",
    "\n",
    "    def prepare_data(self, dataset):\n",
    "        return train_test_split(dataset[0], dataset[1], stratify=dataset[1], test_size=self.test_size)\n",
    "\n",
    "    def compute(self):\n",
    "\n",
    "        print('=========================================================================')\n",
    "        print('=========================     START COMPUTING     =======================')\n",
    "        print('=========================================================================\\n')\n",
    "        print(f'Mode: {self.mode}')\n",
    "        print(f'Iterations: {self.iterations}')\n",
    "        print(f'Oversampling strategies: {self.oversampling_strategies}')\n",
    "        print(f'Metrics: {self.metrics}')\n",
    "        print(f'Number of trees: {self.n_trees}')\n",
    "        print(f'Datasets: {self.dataset_name}')\n",
    "\n",
    "        self.results_bgg = []\n",
    "        self.results_aug = []\n",
    "        self.results_rf = []\n",
    "        self.visualization_data_bgg = []\n",
    "        self.visualization_data_aug = []\n",
    "        for i, dataset in enumerate(self.datasets):\n",
    "            dataset_name = self.dataset_name[i]\n",
    "            print(f'\\n \\n + DATASET: {dataset_name}')\n",
    "            if self.mode == 'both':\n",
    "                bgg_results, bgg_visualization_data = self.compute_bagging(dataset, dataset_name)\n",
    "                aug_results, aug_visualization_data = self.compute_augmentation(dataset, dataset_name)\n",
    "                rf_results = self.compute_baseline(dataset, dataset_name)\n",
    "                self.results_bgg.append(bgg_results)\n",
    "                self.results_aug.append(aug_results)\n",
    "                self.results_rf.append(rf_results)\n",
    "                self.visualization_data_bgg.append(bgg_visualization_data)\n",
    "                self.visualization_data_aug.append(aug_visualization_data)\n",
    "            elif self.mode == 'bagging':\n",
    "                bgg_results, bgg_visualization_data = self.compute_bagging(dataset, dataset_name)\n",
    "                rf_results = self.compute_baseline(dataset, dataset_name)\n",
    "                self.results_rf.append(rf_results)\n",
    "                self.results_bgg.append(bgg_results)\n",
    "                self.visualization_data_bgg.append(bgg_visualization_data)\n",
    "            elif self.mode == 'augmentation':\n",
    "                aug_results, aug_visualization_data = self.compute_augmentation(dataset, dataset_name)\n",
    "                rf_results = self.compute_baseline(dataset, dataset_name)\n",
    "                self.results_rf.append(rf_results)\n",
    "                self.results_aug.append(aug_results)\n",
    "                self.visualization_data_aug.append(aug_visualization_data)\n",
    "            else:\n",
    "                raise ValueError(f\"Mode {self.mode} is not supported. Choose from 'both', 'bagging', or 'augmentation'.\")\n",
    "            \n",
    "        print('\\n=========================================================================')\n",
    "        print('==================     COMPUTING ENDED SUCCESSFULLY      ================')\n",
    "        print('=========================================================================\\n')\n",
    "\n",
    "    def _print_progress_bar(self, iteration, total, prefix='', length=30):\n",
    "        percent = f\"{100 * (iteration / float(total)):.1f}\"\n",
    "        filled_length = int(length * iteration // total)\n",
    "        bar = '█' * filled_length + '-' * (length - filled_length)\n",
    "        print(f'\\r{prefix} |{bar}| {percent}% Complete', end='\\r')\n",
    "        if iteration == total:\n",
    "            print()\n",
    "\n",
    "    def compute_bagging(self, dataset, dataset_name):\n",
    "        print('\\n-=-=-=-=-=-=   BAGGING   =-=-=-=-=-')\n",
    "        visualization_data = []\n",
    "        class_names = np.unique(dataset[1])\n",
    "        if 'accuracy' not in self.metrics:\n",
    "            n_metrics = len(self.metrics) * len(class_names)\n",
    "        else:\n",
    "            n_metrics = (len(self.metrics) - 1) * len(class_names) + 1\n",
    "        results = []\n",
    "\n",
    "        for strategy in self.oversampling_strategies:\n",
    "            strategy_results = [[] for _ in range(n_metrics)]\n",
    "            strategy_visualization_data = []\n",
    "            for j in range(self.iterations):\n",
    "                self._print_progress_bar(j + 1, self.iterations, prefix=f'{strategy} - bagging')\n",
    "                if self.print_indices_list[j] is None:\n",
    "                    indices = None\n",
    "                else:\n",
    "                    indices = self.print_indices_list[j]\n",
    "                \n",
    "                X_train, X_test, y_train, y_test = self.prepare_data(dataset)\n",
    "                forest = OSRandomForestClassifier(\n",
    "                    oversampling_strategy=strategy,\n",
    "                    print_indices_list=indices,\n",
    "                    n_estimators=self.n_trees,\n",
    "                    data_name=dataset_name,\n",
    "                    iteration=j)\n",
    "                forest.fit(X_train, y_train)\n",
    "\n",
    "                if indices is not None:\n",
    "                    for i in indices:\n",
    "                        strategy_visualization_data.append(forest.estimators_[i].visualization_pack)\n",
    "\n",
    "                y_pred = forest.predict(X_test)\n",
    "                report = classification_report(y_test, y_pred, output_dict=True, target_names=[str(c) for c in class_names])\n",
    "                \n",
    "                idx = 0\n",
    "                for cls in class_names:\n",
    "                    for metric in self.metrics:\n",
    "                        if metric == 'accuracy':\n",
    "                            continue\n",
    "                        if metric in report[str(cls)]:\n",
    "                            strategy_results[idx].append(report[str(cls)][metric])\n",
    "                        else:\n",
    "                            raise ValueError(f\"Metric {metric} not found in report for class {cls}.\")\n",
    "                        idx += 1\n",
    "                if 'accuracy' in self.metrics:\n",
    "                    strategy_results[-1].append(report['accuracy'])\n",
    "\n",
    "            results.append(strategy_results)\n",
    "            visualization_data.append(strategy_visualization_data)\n",
    "\n",
    "        return results, visualization_data\n",
    "\n",
    "    def compute_augmentation(self, dataset, dataset_name):\n",
    "        print('\\n-=-=-=-=-=-=   AUGMENTATION   =-=-=-=-=-')\n",
    "        visualization_data = []\n",
    "        class_names = np.unique(dataset[1])\n",
    "        if 'accuracy' not in self.metrics:\n",
    "            n_metrics = len(self.metrics) * len(class_names)\n",
    "        else:\n",
    "            n_metrics = (len(self.metrics) - 1) * len(class_names) + 1\n",
    "        results = []\n",
    "        \n",
    "        for strategy in self.oversampling_strategies:\n",
    "            strategy_results = [[] for _ in range(n_metrics)]\n",
    "            strategy_visualization_data = []\n",
    "            for j in range(self.iterations):\n",
    "                self._print_progress_bar(j + 1, self.iterations, prefix=f'{strategy} - augmentation')\n",
    "\n",
    "                X_train, X_test, y_train, y_test = self.prepare_data(dataset)\n",
    "\n",
    "                if strategy == \"random\":\n",
    "                    sampler = RandomOverSampler()\n",
    "                elif strategy == \"SMOTE\":\n",
    "                    sampler = SMOTE()\n",
    "                elif strategy == \"BorderlineSMOTE\":\n",
    "                    sampler = BorderlineSMOTE()\n",
    "                elif strategy == \"ADASYN\":\n",
    "                    sampler = ADASYN()\n",
    "                else:\n",
    "                    raise ValueError(f\"Oversampling strategy {strategy} is not supported.\")\n",
    "                \n",
    "                forest = RandomForestClassifier(\n",
    "                    n_estimators=self.n_trees\n",
    "                )\n",
    "\n",
    "                X_resampled, y_resampled = sampler.fit_resample(X_train, y_train)\n",
    "\n",
    "                forest.fit(X_resampled, y_resampled)\n",
    "                y_pred = forest.predict(X_test)\n",
    "\n",
    "                if self.print_indices_list[j] is not None:\n",
    "                    strategy_visualization_data.append([X_train, X_resampled, y_resampled, dataset_name, strategy, j])\n",
    "\n",
    "                report = classification_report(y_test, y_pred, output_dict=True, target_names=[str(c) for c in class_names])\n",
    "                idx = 0\n",
    "                for cls in class_names:\n",
    "                    for metric in self.metrics:\n",
    "                        if metric == 'accuracy':\n",
    "                            continue\n",
    "                        if metric in report[str(cls)]:\n",
    "                            strategy_results[idx].append(report[str(cls)][metric])\n",
    "                        else:\n",
    "                            raise ValueError(f\"Metric {metric} not found in report for class {cls}.\")\n",
    "                        idx += 1\n",
    "                if 'accuracy' in self.metrics:\n",
    "                    strategy_results[-1].append(report['accuracy'])\n",
    "            \n",
    "            results.append(strategy_results)\n",
    "            visualization_data.append(strategy_visualization_data)\n",
    "                \n",
    "        return results, visualization_data\n",
    "    \n",
    "    def compute_baseline(self, dataset, dataset_name):\n",
    "        print('\\n-=-=-=-=-=-=   BASELINE   =-=-=-=-=-')\n",
    "        class_names = np.unique(dataset[1])\n",
    "        if 'accuracy' not in self.metrics:\n",
    "            n_metrics = len(self.metrics) * len(class_names)\n",
    "        else:\n",
    "            n_metrics = (len(self.metrics) - 1) * len(class_names) + 1\n",
    "        results = [[] for _ in range(n_metrics)]\n",
    "        for j in range(self.iterations):\n",
    "            self._print_progress_bar(j + 1, self.iterations, prefix='baseline')\n",
    "\n",
    "            X_train, X_test, y_train, y_test = self.prepare_data(dataset)\n",
    "                \n",
    "            forest = RandomForestClassifier(\n",
    "                n_estimators=self.n_trees\n",
    "            )\n",
    "\n",
    "            forest.fit(X_train, y_train)\n",
    "            y_pred = forest.predict(X_test)\n",
    "            report = classification_report(y_test, y_pred, output_dict=True, target_names=[str(c) for c in class_names])\n",
    "            idx = 0\n",
    "            for cls in class_names:\n",
    "                for metric in self.metrics:\n",
    "                    if metric == 'accuracy':\n",
    "                        continue\n",
    "                    if metric in report[str(cls)]:\n",
    "                        results[idx].append(report[str(cls)][metric])\n",
    "                    else:\n",
    "                        raise ValueError(f\"Metric {metric} not found in report for class {cls}.\")\n",
    "                    idx += 1\n",
    "            if 'accuracy' in self.metrics:\n",
    "                results[-1].append(report['accuracy'])\n",
    "                \n",
    "        return results\n",
    "    \n",
    "    def print_table(self, results, class_names, metrics):\n",
    "        idx = 0\n",
    "        for metric in metrics:\n",
    "            if metric == 'accuracy':\n",
    "                vals = np.array(results[-1])\n",
    "                print(f\"\\n{'Accuracy':>12} {'min':>12} {'avg':>12} {'max':>12} {'std':>12}\")\n",
    "                print(f\"{'':>12} {np.min(vals):12.4f} {np.mean(vals):12.4f} {np.max(vals):12.4f} {np.std(vals):12.4f}\")\n",
    "            else:\n",
    "                print(f\"\\n{'':>12} {'min_'+metric:>12} {'avg_'+metric:>12} {'max_'+metric:>12} {'std_'+metric:>12}\")\n",
    "                for cidx, cls in enumerate(class_names):\n",
    "                    vals = np.array(results[idx])\n",
    "                    print(f\"{str(cls):>12} {np.min(vals):12.4f} {np.mean(vals):12.4f} {np.max(vals):12.4f} {np.std(vals):12.4f}\")\n",
    "                    idx += 1\n",
    "\n",
    "    def plot_violin_metrics(self, dataset_index):\n",
    "        dataset_name = self.dataset_name[dataset_index]\n",
    "        if self.mode == 'both':\n",
    "            names = [f\"{name} bagging\" for name in self.oversampling_strategies] + \\\n",
    "                    [f\"{name} augmentation\" for name in self.oversampling_strategies] + \\\n",
    "                    ['baseline']\n",
    "            results = self.results_bgg[dataset_index] + self.results_aug[dataset_index] + [self.results_rf[dataset_index]]\n",
    "        elif self.mode == 'bagging':\n",
    "            names = [f\"{name} bagging\" for name in self.oversampling_strategies] + ['baseline']\n",
    "            results = self.results_bgg[dataset_index] + [self.results_rf[dataset_index]]\n",
    "        elif self.mode == 'augmentation':\n",
    "            names = [f\"{name} augmentation\" for name in self.oversampling_strategies] + ['baseline']\n",
    "            results = self.results_aug[dataset_index] + [self.results_rf[dataset_index]]\n",
    "\n",
    "        labels = np.unique(self.datasets[dataset_index][1])\n",
    "        metrics = self.metrics\n",
    "        n_classes = len(labels)\n",
    "\n",
    "        plot_data = []\n",
    "        plot_labels = []\n",
    "        plot_methods = []\n",
    "        plot_metrics = []\n",
    "        plot_classes = []\n",
    "\n",
    "        idx_metric = 0\n",
    "        for m, metric in enumerate(metrics):\n",
    "            if metric == 'accuracy':\n",
    "                for i, method in enumerate(names):\n",
    "                    vals = np.array(results[i][-1])\n",
    "                    plot_data.extend(vals)\n",
    "                    plot_labels.extend([method] * len(vals))\n",
    "                    plot_methods.extend([method] * len(vals))\n",
    "                    plot_metrics.extend([metric] * len(vals))\n",
    "                    plot_classes.extend(['accuracy'] * len(vals))\n",
    "            else:\n",
    "                for c, cls in enumerate(labels):\n",
    "                    for i, method in enumerate(names):\n",
    "                        vals = np.array(results[i][idx_metric])\n",
    "                        plot_data.extend(vals)\n",
    "                        plot_labels.extend([method] * len(vals))\n",
    "                        plot_methods.extend([method] * len(vals))\n",
    "                        plot_metrics.extend([metric] * len(vals))\n",
    "                        plot_classes.extend([str(cls)] * len(vals))\n",
    "                    idx_metric += 1\n",
    "\n",
    "        df = pd.DataFrame({\n",
    "            'Value': plot_data,\n",
    "            'Method': plot_methods,\n",
    "            'Metric': plot_metrics,\n",
    "            'Class': plot_classes\n",
    "        })\n",
    "        palettes = [[\"#b39ddb\"],\n",
    "                   [\"#ffcc80\"],\n",
    "                   [\"#a5d6a7\"],\n",
    "                   [\"#90caf9\"]]\n",
    "        plot_idx = 0\n",
    "        for m, metric in enumerate(metrics):\n",
    "            if metric == 'accuracy':\n",
    "                plt.figure(figsize=(8, 6))\n",
    "                sns.violinplot(\n",
    "                    data=df[df['Metric'] == 'accuracy'],\n",
    "                    x='Method', y='Value',\n",
    "                    palette=palettes[plot_idx//n_classes]\n",
    "                )\n",
    "                plt.title(f'Accuracy for {dataset_name}')\n",
    "                plt.xlabel(None)\n",
    "                plt.ylabel(None)\n",
    "                plt.xticks(rotation=70)\n",
    "                plt.tight_layout()\n",
    "                plt.show()\n",
    "                plot_idx += 1\n",
    "            else:\n",
    "                for cls in labels:\n",
    "                    plt.figure(figsize=(8, 6))\n",
    "                    sns.violinplot(\n",
    "                    data=df[(df['Metric'] == metric) & (df['Class'] == str(cls))],\n",
    "                    x='Method', y='Value',\n",
    "                    palette=palettes[plot_idx//n_classes]\n",
    "                    )\n",
    "                    plt.title(f'{metric} (class {cls}) for {dataset_name}')\n",
    "                    plt.xticks(rotation=70)\n",
    "                    plt.xlabel(None)\n",
    "                    plt.ylabel(None)\n",
    "                    plt.tight_layout()\n",
    "                    plt.show()\n",
    "                    plot_idx += 1\n",
    "\n",
    "                \n",
    "    def plot_data(self, X_drawn, X_resampled, y_resampled, data_name, oversampling_strategy, iteration, index, type):\n",
    "        X_drawn_tmp = np.array(X_drawn)\n",
    "        X_resampled_tmp = np.array(X_resampled)\n",
    "        y_resampled_tmp = np.array(y_resampled)\n",
    "\n",
    "        if X_resampled_tmp.shape[1] == 1:\n",
    "            X_plot = np.hstack([X_resampled_tmp, np.zeros((X_resampled_tmp.shape[0], 1))])\n",
    "        elif X_resampled_tmp.shape[1] == 2:\n",
    "            X_plot = X_resampled_tmp\n",
    "        else:\n",
    "            umap_model = UMAP(n_components=2)\n",
    "            X_plot = umap_model.fit_transform(X_resampled_tmp)\n",
    "\n",
    "        marker = len(X_drawn_tmp)\n",
    "        classes = np.unique(y_resampled_tmp)\n",
    "\n",
    "        # Prepare colormaps\n",
    "        orig_palette = sns.color_palette(\"magma\", len(classes))\n",
    "        synth_palette = sns.color_palette(\"viridis\", len(classes))\n",
    "\n",
    "        plt.figure(figsize=(7, 5))\n",
    "\n",
    "        # Plot synthetic samples\n",
    "        for idx, cls in enumerate(classes):\n",
    "            synth_mask = (y_resampled_tmp[marker:] == cls)\n",
    "            if np.any(synth_mask):\n",
    "                sns.scatterplot(\n",
    "                    x=X_plot[marker:, 0][synth_mask],\n",
    "                    y=X_plot[marker:, 1][synth_mask],\n",
    "                    color=synth_palette[idx],\n",
    "                    marker=\"X\",\n",
    "                    s=100,\n",
    "                    label=f\"Synthetic class {int(cls)}\",\n",
    "                    linewidth=0.4\n",
    "                )\n",
    "\n",
    "        # Plot original samples\n",
    "        for idx, cls in enumerate(classes):\n",
    "            orig_mask = (y_resampled_tmp[:marker] == cls)\n",
    "            sns.scatterplot(\n",
    "                x=X_plot[:marker, 0][orig_mask],\n",
    "                y=X_plot[:marker, 1][orig_mask],\n",
    "                color=orig_palette[idx],\n",
    "                marker=\"o\",\n",
    "                linewidth=0.4,\n",
    "                s=40,\n",
    "                label=f\"Original class {int(cls)}\"\n",
    "            )\n",
    "\n",
    "        plt.title(f\"{data_name} data after {oversampling_strategy} {type} (and TSNE), passed to tree no. {index} in forest no. {iteration}\")\n",
    "        plt.legend()\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "        print('')\n",
    "        for idx, cls in enumerate(classes):\n",
    "            orig_mask = (y_resampled_tmp[:marker] == cls)\n",
    "            print(f\"Class {int(cls)} has {np.sum(orig_mask)} original samples and {np.sum(y_resampled_tmp[marker:] == cls)} synthetic samples after {oversampling_strategy} oversampling.\")\n",
    "        print('')\n",
    "\n",
    "    def summary(self):\n",
    "        print('\\n', '=' * 73)\n",
    "        print('=========================         SUMMARY         =======================')\n",
    "        print('=' * 73, '\\n')\n",
    "\n",
    "        for i in range(len(self.datasets)):\n",
    "            print('*' * 5, f' DATASET: {self.dataset_name[i]}\\n')\n",
    "            class_names = np.unique(self.datasets[i][1])\n",
    "\n",
    "            if self.mode in ['both', 'bagging']:\n",
    "                for j, strategy in enumerate(self.oversampling_strategies):\n",
    "                    print('\\n', '\\n', f'\\n+++ {strategy} - bagging')\n",
    "                    for v in range(self.visuals):\n",
    "                        self.plot_data(self.visualization_data_bgg[i][j][v][0],\n",
    "                                       self.visualization_data_bgg[i][j][v][1],\n",
    "                                       self.visualization_data_bgg[i][j][v][2],\n",
    "                                       self.visualization_data_bgg[i][j][v][4],\n",
    "                                       self.visualization_data_bgg[i][j][v][3],\n",
    "                                       self.visualization_data_bgg[i][j][v][6],\n",
    "                                       self.visualization_data_bgg[i][j][v][5],\n",
    "                                       'bagging')\n",
    "                    results = self.results_bgg[i][j]\n",
    "                    self.print_table(results, class_names, self.metrics)\n",
    "\n",
    "            if self.mode in ['both', 'augmentation']:\n",
    "                for j, strategy in enumerate(self.oversampling_strategies):\n",
    "                    print(f'\\n \\n+++ {strategy} - augmentation +++')\n",
    "                    self.plot_data(self.visualization_data_aug[i][j][0][0],\n",
    "                                   self.visualization_data_aug[i][j][0][1],\n",
    "                                   self.visualization_data_aug[i][j][0][2],\n",
    "                                   self.visualization_data_aug[i][j][0][3],\n",
    "                                   self.visualization_data_aug[i][j][0][4],\n",
    "                                   self.visualization_data_aug[i][j][0][5],\n",
    "                                   '-', \n",
    "                                   'augmentation')\n",
    "                    results = self.results_aug[i][j]\n",
    "                    self.print_table(results, class_names, self.metrics)\n",
    "\n",
    "            self.print_table(self.results_rf[i], class_names, self.metrics)\n",
    "\n",
    "            self.plot_violin_metrics(i)\n",
    "\n",
    "\n",
    "porownanie = Comparator(\n",
    "    datasets=[[fetch_datasets()['us_crime'].data, fetch_datasets()['us_crime'].target],\n",
    "              [fetch_datasets()['letter_img'].data, fetch_datasets()['letter_img'].target]],\n",
    "    test_size=0.2,\n",
    "    oversampling_strategies=['random', 'SMOTE', 'BorderlineSMOTE', 'ADASYN'],\n",
    "    metrics=['precision', 'recall', 'f1-score', 'accuracy'],\n",
    "    n_trees=2,\n",
    "    iterations=2,\n",
    "    print_indices_list=[[0],[]],\n",
    "    dataset_name=['us_crime', 'letter_img'],\n",
    "    mode='both'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "792380e4",
   "metadata": {},
   "source": [
    "# Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b3469f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========================================================================\n",
      "=========================     START COMPUTING     =======================\n",
      "=========================================================================\n",
      "\n",
      "Mode: both\n",
      "Iterations: 100\n",
      "Oversampling strategies: ['random', 'SMOTE', 'BorderlineSMOTE', 'ADASYN']\n",
      "Metrics: ['precision', 'recall', 'f1-score', 'accuracy']\n",
      "Number of trees: 100\n",
      "Datasets: ['us_crime', 'letter_img']\n",
      "\n",
      " \n",
      " + DATASET: us_crime\n",
      "\n",
      "-=-=-=-=-=-=   BAGGING   =-=-=-=-=-\n",
      "random - bagging |██████████████████████████████| 100.0% Complete\n",
      "SMOTE - bagging |██████████████████████████████| 100.0% Complete\n",
      "BorderlineSMOTE - bagging |████--------------------------| 14.0% Complete\r"
     ]
    }
   ],
   "source": [
    "comparator = Comparator()\n",
    "comparator.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "282f0059",
   "metadata": {},
   "outputs": [],
   "source": [
    "comparator.summary()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
